{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the destination path\n",
    "dst_path = '/p/openvocabdustr/probing_midlevel_vision/code/probing-mid-level-vision/evals/models'\n",
    "\n",
    "# Change to the destination directory\n",
    "os.chdir(dst_path)\n",
    "\n",
    "# Add the destination path to sys.path so that it can import the necessary modules\n",
    "sys.path.append(dst_path)\n",
    "\n",
    "# Now you can import your modules as absolute imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from PIL import Image\n",
    "\n",
    "# Replace relative imports with absolute imports\n",
    "from util import load_checkpoint, prepare_state_dict\n",
    "from utils import center_padding\n",
    "import torchvision\n",
    "\n",
    "# Import CroCoNet from the croco_model\n",
    "from croco_models.croco import CroCoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoints and paths\n",
    "checkpoints = {\n",
    "    \"croco_v2\": {\n",
    "        \"url\": \"https://download.europe.naverlabs.com/ComputerVision/CroCo/CroCo.pth\",  # Replace with actual URL\n",
    "        \"filename\": \"CroCo.pth\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class CroCoV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"croco_v2\",\n",
    "        layer=-1,\n",
    "        output=\"dense\",\n",
    "        return_multilayer=False,\n",
    "        add_norm=False,  # Add flag to control batch normalization\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the model within __init__\n",
    "        self.model = self.load_model(model_name)\n",
    "\n",
    "        self.output = output\n",
    "        self.checkpoint_name = f\"${model_name}$_{output}\"\n",
    "        self.patch_size = 16  # CroCoNet typically uses a 16x16 patch size\n",
    "        self.add_norm = add_norm\n",
    "\n",
    "        # Setup batch normalization layers for each layer\n",
    "        num_layers = len(self.model.enc_blocks)\n",
    "        feat_dim = 512  # Adjust this based on actual feature size in CroCoNet\n",
    "\n",
    "        # Define which layers to extract\n",
    "        multilayers = [\n",
    "            0, 2, 4, 6,\n",
    "        ]\n",
    "\n",
    "        if return_multilayer:\n",
    "            self.feat_dim = [feat_dim] * 4\n",
    "            self.multilayers = multilayers\n",
    "        else:\n",
    "            self.feat_dim = feat_dim\n",
    "            self.multilayers = [multilayers[-1]]\n",
    "        self.layer = \"-\".join(str(_x) for _x in self.multilayers)\n",
    "\n",
    "        # Define BatchNorm2d layers for each multilayer\n",
    "        self.batchnorms = nn.ModuleList(\n",
    "            [nn.BatchNorm2d(feat_dim) for _ in self.multilayers]\n",
    "        )\n",
    "\n",
    "    def load_model(self, model_name: str):\n",
    "        \"\"\"Load the CroCo model from checkpoint.\"\"\"\n",
    "        assert model_name in checkpoints.keys(), f\"Invalid model: {model_name}\"\n",
    "        ckpt = load_checkpoint(**checkpoints[model_name])\n",
    "        # ckpt = prepare_state_dict(ckpt[\"model\"])\n",
    "\n",
    "        model = CroCoNet(\n",
    "            **ckpt.get(\"croco_kwargs\", {})\n",
    "        )  # Initialize CroCoNet with arguments\n",
    "        model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "        return model.eval()\n",
    "\n",
    "    def forward(self, image1, image2):\n",
    "        \"\"\"Forward pass through the CroCo model.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            # encoder of the masked first image\n",
    "            feat1, pos1, mask1 = self.model._encode_image(image1, do_mask=True)\n",
    "            # encoder of the self.model image\n",
    "            feat2, pos2, _ = self.model._encode_image(image2, do_mask=False)\n",
    "            # decoder\n",
    "            visf1 = self.model.decoder_embed(feat1)\n",
    "            f2 = self.model.decoder_embed(feat2)\n",
    "            # append masked tokens to the sequence\n",
    "            outputs = []\n",
    "            \n",
    "            B, Nenc, C = visf1.size()\n",
    "            if mask1 is None: # downstreams\n",
    "                f1_ = visf1\n",
    "            else: # pretraining \n",
    "                Ntotal = mask1.size(1)\n",
    "                f1_ = self.model.mask_token.repeat(B, Ntotal, 1).to(dtype=visf1.dtype)\n",
    "                f1_[~mask1] = visf1.view(B * Nenc, C)\n",
    "            # add positional embedding\n",
    "            if self.model.dec_pos_embed is not None:\n",
    "                f1_ = f1_ + self.model.dec_pos_embed\n",
    "                f2 = f2 + self.model.dec_pos_embed\n",
    "            # apply Transformer blocks\n",
    "            out = f1_\n",
    "            out2 = f2\n",
    "            \n",
    "            for idx, blk in enumerate(self.model.dec_blocks):\n",
    "                out, out2 = blk(out, out2, pos1, pos2)\n",
    "                if idx in self.multilayers:\n",
    "                    if self.add_norm:\n",
    "                        out_norm = self.batchnorms[self.multilayers.index(idx)](out.reshape(1, 14, 14, 512).permute(0, 3, 1, 2))\n",
    "                        out_norm = out_norm.permute(0, 2, 3, 1)\n",
    "                        outputs.append(out_norm)\n",
    "                    else:\n",
    "                        outputs.append(out)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def process_images(self, image_paths, device):\n",
    "        \"\"\"Process input images and apply necessary transformations.\"\"\"\n",
    "        imagenet_mean = [0.485, 0.456, 0.406]\n",
    "        imagenet_std = [0.229, 0.224, 0.225]\n",
    "        trfs = Compose([ToTensor(), Normalize(mean=imagenet_mean, std=imagenet_std)])\n",
    "\n",
    "        # Load and transform images\n",
    "        images = [\n",
    "            trfs(Image.open(image).convert(\"RGB\")).to(device).unsqueeze(0)\n",
    "            for image in image_paths\n",
    "        ]\n",
    "        return images\n",
    "\n",
    "    def decode_output(self, output, image1, mask, device):\n",
    "        \"\"\"Undo normalization and prepare masked image for visualization.\"\"\"\n",
    "        imagenet_mean_tensor = (\n",
    "            torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "        )\n",
    "        imagenet_std_tensor = (\n",
    "            torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "        )\n",
    "\n",
    "        decoded_image = output * imagenet_std_tensor + imagenet_mean_tensor\n",
    "        input_image = image1 * imagenet_std_tensor + imagenet_mean_tensor\n",
    "        image_masks = self.model.unpatchify(\n",
    "            self.model.patchify(torch.ones_like(image1)) * mask[:, :, None]\n",
    "        )\n",
    "        masked_input_image = (1 - image_masks) * input_image\n",
    "\n",
    "        return decoded_image, masked_input_image\n",
    "\n",
    "    def visualize(self, image1, image2, decoded_image, masked_input_image):\n",
    "        \"\"\"Create visualization of the input, reference, masked, and decoded images.\"\"\"\n",
    "        visualization = torch.cat(\n",
    "            (image2, masked_input_image, decoded_image, image1), dim=3\n",
    "        )\n",
    "        B, C, H, W = visualization.shape\n",
    "        visualization = visualization.permute(1, 0, 2, 3).reshape(C, B * H, W)\n",
    "        return torchvision.transforms.functional.to_pil_image(\n",
    "            torch.clamp(visualization, 0, 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CroCoV2(output=\"dense\", add_norm=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process images\n",
    "image_paths = [\"assets/Chateau1.png\", \"assets/Chateau2.png\"]\n",
    "images = model.process_images(image_paths, device)\n",
    "image1, image2 = images\n",
    "\n",
    "# Forward pass\n",
    "features = model.forward(image1, image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 14, 512])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

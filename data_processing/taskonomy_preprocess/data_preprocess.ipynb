{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, Features, Image, Value\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "train_scenes = [\n",
    "    \"hanson\", \"merom\", \"klickitat\", \"onaga\", \"leonardo\", \"marstons\", \"newfields\", \"pinesdale\",\n",
    "    \"lakeville\", \"cosmos\", \"benevolence\", \"pomaria\", \"tolstoy\", \"shelbyville\", \"allensville\",\n",
    "    \"wainscott\", \"beechwood\", \"coffeen\", \"stockman\", \"hiteman\", \"woodbine\", \"lindenwood\",\n",
    "    \"forkland\", \"mifflinburg\", \"ranchester\"\n",
    "]\n",
    "\n",
    "# Validation scenes\n",
    "validation_scenes = [\n",
    "    \"wiconisco\", \"corozal\", \"collierville\", \"markleeville\", \"darden\"\n",
    "]\n",
    "\n",
    "# Test scenes\n",
    "test_scenes = [\n",
    "    \"ihlen\", \"muleshoe\", \"uvalda\", \"noxapater\", \"mcdade\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, Features, Image, Value\n",
    "\n",
    "def load_and_sample_dataset(input_dir_rgb, output_dir_depth, output_dir_reshading, output_dir_edge_occlusion,\n",
    "                            output_dir_edge_texture, output_dir_keypoints2d, output_dir_keypoints3d,\n",
    "                            output_dir_curvature, output_dir_normal, output_dir_mask_valid,\n",
    "                            train_sample_size, val_sample_size, test_sample_size):\n",
    "    \n",
    "    # Initialize data structures for train, validation, and test\n",
    "    train_data = {\n",
    "        'rgb': [], 'depth': [], 'reshading': [], 'edge_occlusion': [], 'edge_texture': [],\n",
    "        'keypoints2d': [], 'keypoints3d': [], 'principal_curvature': [], 'mask_valid': [], 'scene': []\n",
    "    }\n",
    "    \n",
    "    val_data = {\n",
    "        'rgb': [], 'depth': [], 'reshading': [], 'edge_occlusion': [], 'edge_texture': [],\n",
    "        'keypoints2d': [], 'keypoints3d': [], 'principal_curvature': [], 'mask_valid': [], 'scene': []\n",
    "    }\n",
    "\n",
    "    test_data = {\n",
    "        'rgb': [], 'depth': [], 'reshading': [], 'edge_occlusion': [], 'edge_texture': [],\n",
    "        'keypoints2d': [], 'keypoints3d': [], 'principal_curvature': [], 'mask_valid': [], 'scene': []\n",
    "    }\n",
    "\n",
    "    # Sample size per scene\n",
    "    train_sample_per_scene = train_sample_size // len(train_scenes) * 2\n",
    "    val_sample_per_scene = val_sample_size // len(validation_scenes) * 2\n",
    "    test_sample_per_scene = test_sample_size // len(test_scenes) * 2\n",
    "\n",
    "    # Total number of images to be collected\n",
    "    total_images_to_collect = train_sample_size + val_sample_size + test_sample_size\n",
    "    collected_images = 0  # Track the number of images collected\n",
    "\n",
    "    # Create a tqdm progress bar for the total number of images to collect\n",
    "    pbar = tqdm(total=total_images_to_collect, desc=\"Collecting images\", unit=\"img\")\n",
    "\n",
    "    # Track images per scene\n",
    "    scene_image_count = {scene: 0 for scene in train_scenes + validation_scenes + test_scenes}\n",
    "\n",
    "    # Traverse through the directories for keypoints3d images (use keypoints3d as reference)\n",
    "    for root, _, files in os.walk(output_dir_keypoints3d):\n",
    "        random.shuffle(files)  # Shuffle to randomize sampling\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                # Get the relative path from the keypoints3d directory\n",
    "                relative_path = os.path.relpath(os.path.join(root, file), output_dir_keypoints3d)\n",
    "\n",
    "                # Build paths for all other outputs using the relative path of keypoints3d\n",
    "                rgb_path = os.path.join(input_dir_rgb, relative_path.replace('keypoints3d', 'rgb'))\n",
    "                depth_path = os.path.join(output_dir_depth, relative_path.replace('keypoints3d', 'depth_euclidean'))\n",
    "                reshading_path = os.path.join(output_dir_reshading, relative_path.replace('keypoints3d', 'reshading'))\n",
    "                edge_occlusion_path = os.path.join(output_dir_edge_occlusion, relative_path.replace('keypoints3d', 'edge_occlusion'))\n",
    "                edge_texture_path = os.path.join(output_dir_edge_texture, relative_path.replace('keypoints3d', 'edge_texture'))\n",
    "                keypoints2d_path = os.path.join(output_dir_keypoints2d, relative_path.replace('keypoints3d', 'keypoints2d'))\n",
    "                curvature_path = os.path.join(output_dir_curvature, relative_path.replace('keypoints3d', 'principal_curvature'))\n",
    "                # normal_path = os.path.join(output_dir_normal, relative_path.replace('keypoints3d', 'normal'))\n",
    "                mask_valid_path = os.path.join(output_dir_mask_valid, relative_path.replace('keypoints3d', 'depth_zbuffer'))\n",
    "                \n",
    "                # Check if all corresponding files exist\n",
    "                if all(os.path.exists(path) for path in [rgb_path, depth_path, reshading_path, edge_occlusion_path, \n",
    "                                                         edge_texture_path, keypoints2d_path, curvature_path, mask_valid_path]):\n",
    "                    # Determine scene name\n",
    "                    scene_name = relative_path.split('/')[0]  # Assuming first folder is the scene name\n",
    "\n",
    "                    # Sample images for train, validation, and test based on scene\n",
    "                    if scene_name in train_scenes and scene_image_count[scene_name] < train_sample_per_scene:\n",
    "                        train_data['rgb'].append(rgb_path)\n",
    "                        train_data['depth'].append(depth_path)\n",
    "                        train_data['reshading'].append(reshading_path)\n",
    "                        train_data['edge_occlusion'].append(edge_occlusion_path)\n",
    "                        train_data['edge_texture'].append(edge_texture_path)\n",
    "                        train_data['keypoints2d'].append(keypoints2d_path)\n",
    "                        train_data['keypoints3d'].append(os.path.join(root, file))\n",
    "                        train_data['principal_curvature'].append(curvature_path)\n",
    "                        # train_data['normal'].append(normal_path)\n",
    "                        train_data['mask_valid'].append(mask_valid_path)\n",
    "                        train_data['scene'].append(scene_name)\n",
    "                        scene_image_count[scene_name] += 1\n",
    "                        collected_images += 1  # Update collected image count\n",
    "                        pbar.update(1)  # Update tqdm bar by 1\n",
    "\n",
    "                    elif scene_name in validation_scenes and scene_image_count[scene_name] < val_sample_per_scene:\n",
    "                        val_data['rgb'].append(rgb_path)\n",
    "                        val_data['depth'].append(depth_path)\n",
    "                        val_data['reshading'].append(reshading_path)\n",
    "                        val_data['edge_occlusion'].append(edge_occlusion_path)\n",
    "                        val_data['edge_texture'].append(edge_texture_path)\n",
    "                        val_data['keypoints2d'].append(keypoints2d_path)\n",
    "                        val_data['keypoints3d'].append(os.path.join(root, file))\n",
    "                        val_data['principal_curvature'].append(curvature_path)\n",
    "                        # val_data['normal'].append(normal_path)\n",
    "                        val_data['mask_valid'].append(mask_valid_path)\n",
    "                        val_data['scene'].append(scene_name)\n",
    "                        scene_image_count[scene_name] += 1\n",
    "                        collected_images += 1  # Update collected image count\n",
    "                        pbar.update(1)  # Update tqdm bar by 1\n",
    "\n",
    "                    elif scene_name in test_scenes and scene_image_count[scene_name] < test_sample_per_scene:\n",
    "                        test_data['rgb'].append(rgb_path)\n",
    "                        test_data['depth'].append(depth_path)\n",
    "                        test_data['reshading'].append(reshading_path)\n",
    "                        test_data['edge_occlusion'].append(edge_occlusion_path)\n",
    "                        test_data['edge_texture'].append(edge_texture_path)\n",
    "                        test_data['keypoints2d'].append(keypoints2d_path)\n",
    "                        test_data['keypoints3d'].append(os.path.join(root, file))\n",
    "                        test_data['principal_curvature'].append(curvature_path)\n",
    "                        # test_data['normal'].append(normal_path)\n",
    "                        test_data['mask_valid'].append(mask_valid_path)\n",
    "                        test_data['scene'].append(scene_name)\n",
    "                        scene_image_count[scene_name] += 1\n",
    "                        collected_images += 1  # Update collected image count\n",
    "                        pbar.update(1)  # Update tqdm bar by 1\n",
    "\n",
    "    pbar.close()  # Close the progress bar\n",
    "\n",
    "    # Define the feature structure of the dataset (loading image files)\n",
    "    features = Features({\n",
    "        'rgb': Image(),         # Load RGB images\n",
    "        'depth': Image(),       # Load Depth images\n",
    "        'reshading': Image(),   # Load Reshading images\n",
    "        'edge_occlusion': Image(),\n",
    "        'edge_texture': Image(),\n",
    "        'keypoints2d': Image(),\n",
    "        'keypoints3d': Image(),\n",
    "        'principal_curvature': Image(),\n",
    "        'mask_valid': Image(),\n",
    "        'scene': Value(\"string\") # Use the correct type for string values\n",
    "    })\n",
    "\n",
    "    # Create train, validation, and test datasets using the collected data\n",
    "    train_dataset = Dataset.from_dict(train_data, features=features)\n",
    "    val_dataset = Dataset.from_dict(val_data, features=features)\n",
    "    test_dataset = Dataset.from_dict(test_data, features=features)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_rgb = '/p/openvocabdustr/probing_midlevel_vision/data/rgb/taskonomy'\n",
    "output_dir_depth = '/p/openvocabdustr/probing_midlevel_vision/data/depth_euclidean/taskonomy'\n",
    "output_dir_reshading = '/p/openvocabdustr/probing_midlevel_vision/data/reshading/taskonomy'\n",
    "output_dir_edge_occulusion = '/p/openvocabdustr/probing_midlevel_vision/data/edge_occlusion/taskonomy'\n",
    "output_dir_edge_texture = '/p/openvocabdustr/probing_midlevel_vision/data/edge_texture/taskonomy'\n",
    "output_dir_keypoints2d = '/p/openvocabdustr/probing_midlevel_vision/data/keypoints2d/taskonomy'\n",
    "output_dir_keypoints3d = '/p/openvocabdustr/probing_midlevel_vision/data/keypoints3d/taskonomy'\n",
    "output_dir_curvature = '/p/openvocabdustr/probing_midlevel_vision/data/principal_curvature/taskonomy'\n",
    "output_dir_normal = '/p/openvocabdustr/probing_midlevel_vision/data/normal/taskonomy'\n",
    "output_dir_mask_valid = '/p/openvocabdustr/probing_midlevel_vision/data/mask_valid/taskonomy'\n",
    "\n",
    "# Set the sample size for train, validation, and test sets\n",
    "train_sample_size = 20000\n",
    "val_sample_size = 2000\n",
    "test_sample_size = 2000\n",
    "\n",
    "# Load and sample the custom dataset\n",
    "train_dataset, val_dataset, test_dataset = load_and_sample_dataset(input_dir_rgb, output_dir_depth, output_dir_reshading, \n",
    "                                                      output_dir_edge_occulusion, output_dir_edge_texture, \n",
    "                                                      output_dir_keypoints2d, output_dir_keypoints3d, \n",
    "                                                      output_dir_curvature, output_dir_normal, output_dir_mask_valid,\n",
    "                                                      train_sample_size, val_sample_size, test_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access train and test datasets\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "# Count occurrences of each scene in the train, validation, and test datasets\n",
    "train_scene_distribution = collections.Counter(train_dataset['scene'])\n",
    "val_scene_distribution = collections.Counter(val_dataset['scene'])\n",
    "test_scene_distribution = collections.Counter(test_dataset['scene'])\n",
    "\n",
    "# Convert the scene distributions to DataFrames for easier plotting\n",
    "train_df = pd.DataFrame(train_scene_distribution.items(), columns=['Scene', 'Count'])\n",
    "val_df = pd.DataFrame(val_scene_distribution.items(), columns=['Scene', 'Count'])\n",
    "test_df = pd.DataFrame(test_scene_distribution.items(), columns=['Scene', 'Count'])\n",
    "\n",
    "# Add a 'Type' column to differentiate between Train, Validation, and Test\n",
    "train_df['Type'] = 'Train'\n",
    "val_df['Type'] = 'Validation'\n",
    "test_df['Type'] = 'Test'\n",
    "\n",
    "# Combine all three datasets into a single DataFrame for plotting\n",
    "combined_df = pd.concat([train_df, val_df, test_df])\n",
    "\n",
    "# Plot the distribution of train, validation, and test samples\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot different colors for Train, Validation, and Test scenes\n",
    "plt.bar(train_df[\"Scene\"], train_df[\"Count\"], color='blue', label=\"Train\")\n",
    "plt.bar(val_df[\"Scene\"], val_df[\"Count\"], color='orange', label=\"Validation\")\n",
    "plt.bar(test_df[\"Scene\"], test_df[\"Count\"], color='green', label=\"Test\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Scene\")\n",
    "plt.ylabel(\"Sampled Images Count\")\n",
    "plt.title(\"Sampled Image Distribution: Train, Validation, and Test Sets\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the scene distribution for train, validation, and test datasets\n",
    "print(\"Train scene distribution:\", train_scene_distribution)\n",
    "print(\"Validation scene distribution:\", val_scene_distribution)\n",
    "print(\"Test scene distribution:\", test_scene_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, Features, Image, Value\n",
    "\n",
    "def load_and_sample_rgb_normal_mask_valid(input_dir_rgb, output_dir_normal, output_dir_mask_valid,\n",
    "                                          train_sample_size, val_sample_size, test_sample_size):\n",
    "    \n",
    "    # Initialize data structures for train, validation, and test\n",
    "    train_data = {\n",
    "        'rgb': [], 'normal': [], 'mask_valid': [], 'scene': []\n",
    "    }\n",
    "    \n",
    "    val_data = {\n",
    "        'rgb': [], 'normal': [], 'mask_valid': [], 'scene': []\n",
    "    }\n",
    "\n",
    "    test_data = {\n",
    "        'rgb': [], 'normal': [], 'mask_valid': [], 'scene': []\n",
    "    }\n",
    "\n",
    "    # Sample size per scene\n",
    "    train_sample_per_scene = train_sample_size // len(train_scenes) * 4\n",
    "    val_sample_per_scene = val_sample_size // len(validation_scenes) * 4\n",
    "    test_sample_per_scene = test_sample_size // len(test_scenes) * 4\n",
    "\n",
    "    # Total number of images to be collected\n",
    "    total_images_to_collect = train_sample_size + val_sample_size + test_sample_size\n",
    "    collected_images = 0  # Track the number of images collected\n",
    "\n",
    "    # Create a tqdm progress bar for the total number of images to collect\n",
    "    pbar = tqdm(total=total_images_to_collect, desc=\"Collecting images\", unit=\"img\")\n",
    "\n",
    "    # Track images per scene\n",
    "    scene_image_count = {scene: 0 for scene in train_scenes + validation_scenes + test_scenes}\n",
    "\n",
    "    # Traverse through the directories for normal images (use normal images as reference)\n",
    "    for root, _, files in os.walk(output_dir_normal):\n",
    "        random.shuffle(files)  # Shuffle to randomize sampling\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                # Get the relative path from the normal directory\n",
    "                relative_path = os.path.relpath(os.path.join(root, file), output_dir_normal)\n",
    "\n",
    "                # Build paths for RGB and mask_valid using the relative path of normal\n",
    "                rgb_path = os.path.join(input_dir_rgb, relative_path.replace('normal', 'rgb'))\n",
    "                mask_valid_path = os.path.join(output_dir_mask_valid, relative_path.replace('normal', 'depth_zbuffer'))\n",
    "                \n",
    "                # Check if all corresponding files exist (RGB and mask_valid)\n",
    "                if all(os.path.exists(path) for path in [rgb_path, mask_valid_path]):\n",
    "                    # Determine scene name\n",
    "                    scene_name = relative_path.split('/')[0]  # Assuming first folder is the scene name\n",
    "\n",
    "                    # Sample images for train, validation, and test based on scene\n",
    "                    if scene_name in train_scenes and scene_image_count[scene_name] < train_sample_per_scene:\n",
    "                        train_data['rgb'].append(rgb_path)\n",
    "                        train_data['normal'].append(os.path.join(root, file))\n",
    "                        train_data['mask_valid'].append(mask_valid_path)\n",
    "                        train_data['scene'].append(scene_name)\n",
    "                        scene_image_count[scene_name] += 1\n",
    "                        collected_images += 1  # Update collected image count\n",
    "                        pbar.update(1)  # Update tqdm bar by 1\n",
    "\n",
    "                    elif scene_name in validation_scenes and scene_image_count[scene_name] < val_sample_per_scene:\n",
    "                        val_data['rgb'].append(rgb_path)\n",
    "                        val_data['normal'].append(os.path.join(root, file))\n",
    "                        val_data['mask_valid'].append(mask_valid_path)\n",
    "                        val_data['scene'].append(scene_name)\n",
    "                        scene_image_count[scene_name] += 1\n",
    "                        collected_images += 1  # Update collected image count\n",
    "                        pbar.update(1)  # Update tqdm bar by 1\n",
    "\n",
    "                    elif scene_name in test_scenes and scene_image_count[scene_name] < test_sample_per_scene:\n",
    "                        test_data['rgb'].append(rgb_path)\n",
    "                        test_data['normal'].append(os.path.join(root, file))\n",
    "                        test_data['mask_valid'].append(mask_valid_path)\n",
    "                        test_data['scene'].append(scene_name)\n",
    "                        scene_image_count[scene_name] += 1\n",
    "                        collected_images += 1  # Update collected image count\n",
    "                        pbar.update(1)  # Update tqdm bar by 1\n",
    "\n",
    "    pbar.close()  # Close the progress bar\n",
    "\n",
    "    # Define the feature structure of the dataset (loading image files)\n",
    "    features = Features({\n",
    "        'rgb': Image(),         # Load RGB images\n",
    "        'normal': Image(),      # Load normal images\n",
    "        'mask_valid': Image(),  # Load mask_valid images\n",
    "        'scene': Value(\"string\") # Use the correct type for string values\n",
    "    })\n",
    "\n",
    "    # Create train, validation, and test datasets using the collected data\n",
    "    train_dataset = Dataset.from_dict(train_data, features=features)\n",
    "    val_dataset = Dataset.from_dict(val_data, features=features)\n",
    "    test_dataset = Dataset.from_dict(test_data, features=features)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_rgb = '/p/openvocabdustr/probing_midlevel_vision/data/rgb/taskonomy'\n",
    "output_dir_normal = '/p/openvocabdustr/probing_midlevel_vision/data/normal/taskonomy'\n",
    "output_dir_mask_valid = '/p/openvocabdustr/probing_midlevel_vision/data/mask_valid/taskonomy'\n",
    "\n",
    "# Set the sample size for train, validation, and test sets\n",
    "train_sample_size = 20000\n",
    "val_sample_size = 2000\n",
    "test_sample_size = 2000\n",
    "\n",
    "# Load and sample the custom dataset with only RGB, Normal, and Mask_Valid\n",
    "train_normal_dataset, val_normal_dataset, test_normal_dataset = load_and_sample_rgb_normal_mask_valid(input_dir_rgb, output_dir_normal, \n",
    "                                                      output_dir_mask_valid, train_sample_size, val_sample_size, \n",
    "                                                      test_sample_size)\n",
    "\n",
    "# Check the sizes of the datasets\n",
    "print(f\"Train dataset size: {len(train_normal_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_normal_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_normal_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_parquet(\"train_dataset.parquet\")\n",
    "val_dataset.to_parquet(\"val_dataset.parquet\")\n",
    "test_dataset.to_parquet(\"test_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "\n",
    "# Push the Parquet files to the same dataset repo\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"train_dataset.parquet\",\n",
    "    path_in_repo=\"train_dataset.parquet\",  # The name of the file in the repo\n",
    "    repo_id=\"Xuweiyi/ssl_probing_taskonomy\",  # Replace with your repo\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"val_dataset.parquet\",\n",
    "    path_in_repo=\"val_dataset.parquet\",  # The name of the file in the repo\n",
    "    repo_id=\"Xuweiyi/ssl_probing_taskonomy\",  # Replace with your repo\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"test_dataset.parquet\",\n",
    "    path_in_repo=\"test_dataset.parquet\",  # The name of the file in the repo\n",
    "    repo_id=\"Xuweiyi/ssl_probing_taskonomy\",  # Replace with your repo\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_dataset.to_parquet(\"train_normal_dataset.parquet\")\n",
    "val_normal_dataset.to_parquet(\"val_normal_dataset.parquet\")\n",
    "test_normal_dataset.to_parquet(\"test_normal_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "\n",
    "# Push the Parquet files to the same dataset repo\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"train_normal_dataset.parquet\",\n",
    "    path_in_repo=\"train_dataset.parquet\",  # The name of the file in the repo\n",
    "    repo_id=\"Xuweiyi/ssl_probing_taskonomy_snorm\",  # Replace with your repo\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"val_normal_dataset.parquet\",\n",
    "    path_in_repo=\"val_dataset.parquet\",  # The name of the file in the repo\n",
    "    repo_id=\"Xuweiyi/ssl_probing_taskonomy_snorm\",  # Replace with your repo\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"test_normal_dataset.parquet\",\n",
    "    path_in_repo=\"test_dataset.parquet\",  # The name of the file in the repo\n",
    "    repo_id=\"Xuweiyi/ssl_probing_taskonomy_snorm\",  # Replace with your repo\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the train split from the Hugging Face Hub\n",
    "train_normal_dataset = load_dataset(\"/p/openvocabdustr/probing_midlevel_vision/data/probing_ssl_snorm\", split=\"train\")\n",
    "\n",
    "# Load the test split from the Hugging Face Hub\n",
    "val_normal_dataset = load_dataset(\"/p/openvocabdustr/probing_midlevel_vision/data/probing_ssl_snorm\", split=\"validation\")\n",
    "\n",
    "# Load the test split from the Hugging Face Hub\n",
    "test_normal_dataset = load_dataset(\"/p/openvocabdustr/probing_midlevel_vision/data/probing_ssl_snorm\", split=\"test\")\n",
    "\n",
    "# Access the train and test datasets\n",
    "print(f\"Train dataset size: {len(train_normal_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_normal_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_normal_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n",
    "\n",
    "# Initialize the OneFormer model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_large\")\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_large\").to(device)\n",
    "\n",
    "def add_panoptic_map_and_id2label(example):\n",
    "    \"\"\"\n",
    "    Process each example by running panoptic segmentation and adding the panoptic map and id2label to the dataset.\n",
    "    \"\"\"\n",
    "    # Extract the RGB image directly from the example\n",
    "    image = example['rgb']\n",
    "\n",
    "    # Preprocess the image and run inference\n",
    "    inputs = processor(image, [\"panoptic\"], return_tensors=\"pt\", do_reduce_labels=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Run post-processing step to get panoptic segmentation\n",
    "    panoptic_result = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "\n",
    "    # Extract the panoptic map and segment information\n",
    "    panoptic_map = panoptic_result[\"segmentation\"].cpu().numpy()\n",
    "\n",
    "    # Create the id2label dictionary\n",
    "    id2label = {}\n",
    "    for segment in panoptic_result[\"segments_info\"]:\n",
    "        segment_id = segment['id']\n",
    "        area = np.sum(panoptic_map == segment_id)\n",
    "\n",
    "        # Save id2label mapping with computed area\n",
    "        id2label[segment_id] = {\n",
    "            \"label_id\": segment['label_id'],\n",
    "            \"was_fused\": segment.get('was_fused', False),\n",
    "            \"score\": segment.get('score', 0),\n",
    "            \"area\": area\n",
    "        }\n",
    "\n",
    "    # Add the panoptic_map and id2label to the dataset example\n",
    "    example['panoptic_map'] = panoptic_map\n",
    "    example['id2label'] = id2label\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_dataset = train_normal_dataset.map(add_panoptic_map_and_id2label)\n",
    "val_normal_dataset = val_normal_dataset.map(add_panoptic_map_and_id2label)\n",
    "test_normal_dataset = test_normal_dataset.map(add_panoptic_map_and_id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the correct directory to sys.path\n",
    "# Change directory to the desired path\n",
    "os.chdir(\"/p/openvocabdustr/probing_midlevel_vision/code/probing-mid-level-vision/data_processing/taskonomy_preprocess\")\n",
    "\n",
    "# Add the current directory to sys.path\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# Use absolute import\n",
    "from transforms import task_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_transform_wrapper(example):\n",
    "    # Apply transformations to 'rgb', 'normal', and 'mask_valid'\n",
    "    if 'rgb' in example:\n",
    "        example['rgb_processed'] = task_transform(example['rgb'], 'rgb')\n",
    "    if 'normal' in example:\n",
    "        example['normal_processed'] = task_transform(example['normal'], 'normal')\n",
    "    if 'mask_valid' in example:\n",
    "        example['mask_valid_processed'] = task_transform(example['mask_valid'], 'mask_valid')\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_dataset = train_normal_dataset.map(\n",
    "    task_transform_wrapper,\n",
    "    num_proc=12,  # Use multiple processes for efficiency\n",
    "    desc=\"Processing Train Set: Transforming and Saving New Keys\",\n",
    "    batch_size=1000,  # Process in batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

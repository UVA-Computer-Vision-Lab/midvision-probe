{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def extract_from_mat_no_normalization(mat_file, output_dir, bad_files_log):\n",
    "    \"\"\"\n",
    "    Extracts image, depth, and surface normals from a .mat file without normalization.\n",
    "    Saves the image as PNG and depth and normals as .npy files for exact value preservation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create necessary output directories if they don't exist\n",
    "        output_dir = Path(output_dir)\n",
    "        img_dir = output_dir / \"images\"\n",
    "        depth_dir = output_dir / \"depths\"\n",
    "        norm_dir = output_dir / \"normals\"\n",
    "        \n",
    "        img_dir.mkdir(parents=True, exist_ok=True)\n",
    "        depth_dir.mkdir(parents=True, exist_ok=True)\n",
    "        norm_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Use the filename without extension for saving\n",
    "        base_name = Path(mat_file).stem\n",
    "\n",
    "        # Check if the files already exist\n",
    "        img_path = img_dir / f\"{base_name}_image.png\"\n",
    "        depth_path = depth_dir / f\"{base_name}_depth.npy\"\n",
    "        norm_path = norm_dir / f\"{base_name}_norm.npy\"\n",
    "\n",
    "        # Skip processing if all files already exist\n",
    "        if img_path.exists() and depth_path.exists() and norm_path.exists():\n",
    "            print(f\"Files for {base_name} already exist, skipping processing.\")\n",
    "            return None\n",
    "\n",
    "        # Load the .mat file\n",
    "        instance = scipy.io.loadmat(mat_file)\n",
    "\n",
    "        # Extract the required fields\n",
    "        image = instance['img']  # Do not alter dimensions\n",
    "        depth = instance['depth']  # Depth map, no normalization\n",
    "        snorm = instance['norm']  # Surface normals, no normalization\n",
    "\n",
    "        # Save the image as PNG\n",
    "        Image.fromarray(image.astype(np.uint8)).save(img_path)\n",
    "\n",
    "        # Save the depth as .npy to preserve raw values\n",
    "        np.save(depth_path, depth)\n",
    "\n",
    "        # Save the surface normals as .npy to preserve raw values\n",
    "        np.save(norm_path, snorm)\n",
    "\n",
    "        return {\n",
    "            \"image\": img_path,\n",
    "            \"depth\": depth_path,\n",
    "            \"norm\": norm_path,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the file as a bad one\n",
    "        with open(bad_files_log, 'a') as log_file:\n",
    "            log_file.write(f\"Error processing {mat_file}: {e}\\n\")\n",
    "        print(f\"Error processing {mat_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_directory_no_normalization(mat_dir, output_dir, bad_files_log=\"bad_mat_files.log\"):\n",
    "    \"\"\"\n",
    "    Processes all .mat files in a directory and extracts image, depth, and normals without normalization.\n",
    "    Skips files that have already been processed and records bad .mat files.\n",
    "    \"\"\"\n",
    "    mat_files = [f for f in os.listdir(mat_dir) if f.endswith(\".mat\")]\n",
    "    extracted_data = []\n",
    "\n",
    "    for mat_file in mat_files:\n",
    "        mat_path = os.path.join(mat_dir, mat_file)\n",
    "        extracted = extract_from_mat_no_normalization(mat_path, output_dir, bad_files_log)\n",
    "        if extracted:  # Only add to the list if processing was done\n",
    "            extracted_data.append(extracted)\n",
    "\n",
    "    print(f\"Processed {len(mat_files)} files.\")\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_directory = \"/p/openvocabdustr/probing_midlevel_vision/code/probing-mid-level-vision/data/nyu_geonet\"  # Directory containing .mat files\n",
    "output_directory = \"/p/openvocabdustr/probing_midlevel_vision/code/probing-mid-level-vision/data/nyu_geonet_hf_datasets\"  # Directory for saving extracted files\n",
    "\n",
    "process_directory_no_normalization(mat_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def extract_from_mat_no_normalization(mat_file, output_dir, bad_files_log):\n",
    "    \"\"\"\n",
    "    Extracts image, depth, and surface normals from a .mat file without normalization.\n",
    "    Saves the image as PNG and depth and normals as .npy files for exact value preservation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create necessary output directories if they don't exist\n",
    "        output_dir = Path(output_dir)\n",
    "        img_dir = output_dir / \"images\"\n",
    "        depth_dir = output_dir / \"depths\"\n",
    "        norm_dir = output_dir / \"normals\"\n",
    "        \n",
    "        img_dir.mkdir(parents=True, exist_ok=True)\n",
    "        depth_dir.mkdir(parents=True, exist_ok=True)\n",
    "        norm_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Use the filename without extension for saving\n",
    "        base_name = Path(mat_file).stem\n",
    "\n",
    "        # Check if the files already exist\n",
    "        img_path = img_dir / f\"{base_name}_image.png\"\n",
    "        depth_path = depth_dir / f\"{base_name}_depth.npy\"\n",
    "        norm_path = norm_dir / f\"{base_name}_norm.npy\"\n",
    "\n",
    "        # Skip processing if all files already exist\n",
    "        if img_path.exists() and depth_path.exists() and norm_path.exists():\n",
    "            print(f\"Files for {base_name} already exist, skipping processing.\")\n",
    "            return None\n",
    "\n",
    "        # Load the .mat file\n",
    "        instance = scipy.io.loadmat(mat_file)\n",
    "\n",
    "        # Extract the required fields\n",
    "        image = instance['img']  # Do not alter dimensions\n",
    "        image[:, :, 0] = image[:, :, 0] + 2 * 122.175\n",
    "        image[:, :, 1] = image[:, :, 1] + 2 * 116.169\n",
    "        image[:, :, 2] = image[:, :, 2] + 2 * 103.508\n",
    "        image = image.astype(np.uint8)\n",
    "        depth = instance['depth']  # Depth map, no normalization\n",
    "        snorm = instance['norm']  # Surface normals, no normalization\n",
    "\n",
    "        # Save the image as PNG\n",
    "        Image.fromarray(image.astype(np.uint8)).save(img_path)\n",
    "\n",
    "        # Save the depth as .npy to preserve raw values\n",
    "        np.save(depth_path, depth)\n",
    "\n",
    "        # Save the surface normals as .npy to preserve raw values\n",
    "        np.save(norm_path, snorm)\n",
    "\n",
    "        return {\n",
    "            \"image\": img_path,\n",
    "            \"depth\": depth_path,\n",
    "            \"norm\": norm_path,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the file as a bad one\n",
    "        with open(bad_files_log, 'a') as log_file:\n",
    "            log_file.write(f\"Error processing {mat_file}: {e}\\n\")\n",
    "        print(f\"Error processing {mat_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_directory_no_normalization(mat_dir, output_dir, bad_files_log=\"bad_mat_files.log\", num_workers=4):\n",
    "    \"\"\"\n",
    "    Processes all .mat files in a directory and extracts image, depth, and normals without normalization.\n",
    "    Skips files that have already been processed and records bad .mat files.\n",
    "    Parallelized using ProcessPoolExecutor.\n",
    "    \"\"\"\n",
    "    mat_files = [f for f in os.listdir(mat_dir) if f.endswith(\".mat\")]\n",
    "    extracted_data = []\n",
    "\n",
    "    # Parallel processing with ProcessPoolExecutor\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = {executor.submit(extract_from_mat_no_normalization, os.path.join(mat_dir, mat_file), output_dir, bad_files_log): mat_file for mat_file in mat_files}\n",
    "        \n",
    "        # Process files as they complete\n",
    "        for future in as_completed(futures):\n",
    "            mat_file = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    extracted_data.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {mat_file}: {e}\")\n",
    "\n",
    "    print(f\"Processed {len(mat_files)} files.\")\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_directory = \"/p/openvocabdustr/probing_midlevel_vision/code/probing-mid-level-vision/data/nyu_geonet\"  # Directory containing .mat files\n",
    "output_directory = \"/p/openvocabdustr/probing_midlevel_vision/code/probing-mid-level-vision/data/nyu_geonet_hf_datasets_v2\"  # Directory for saving extracted files\n",
    "bad_files_log = \"bad_mat_files.log\"\n",
    "\n",
    "# Process files in parallel using 4 workers\n",
    "process_directory_no_normalization(mat_directory, output_directory, bad_files_log, num_workers=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def extract_nyuv2_test_data(test_set, output_dir, bad_files_log):\n",
    "    \"\"\"\n",
    "    Extracts image, depth, surface normals, room type, and NYU index from the NYUv2 test set without normalization.\n",
    "    Saves the image as PNG and depth and normals as .npy files for exact value preservation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create necessary output directories if they don't exist\n",
    "        output_dir = Path(output_dir)\n",
    "        img_dir = output_dir / \"images\"\n",
    "        depth_dir = output_dir / \"depths\"\n",
    "        norm_dir = output_dir / \"normals\"\n",
    "        meta_dir = output_dir / \"metadata\"\n",
    "\n",
    "        img_dir.mkdir(parents=True, exist_ok=True)\n",
    "        depth_dir.mkdir(parents=True, exist_ok=True)\n",
    "        norm_dir.mkdir(parents=True, exist_ok=True)\n",
    "        meta_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Process each sample in the test set with tqdm\n",
    "        for index, (image, depth, snorm, room, nyu_index) in tqdm(enumerate(test_set), total=len(test_set), desc=\"Processing test set\"):\n",
    "            base_name = f\"nyuv2_test_{index}\"\n",
    "\n",
    "            # Check if the files already exist\n",
    "            img_path = img_dir / f\"{base_name}_image.png\"\n",
    "            depth_path = depth_dir / f\"{base_name}_depth.npy\"\n",
    "            norm_path = norm_dir / f\"{base_name}_norm.npy\"\n",
    "            meta_path = meta_dir / f\"{base_name}_metadata.npy\"\n",
    "\n",
    "            if img_path.exists() and depth_path.exists() and norm_path.exists() and meta_path.exists():\n",
    "                print(f\"Files for {base_name} already exist, skipping processing.\")\n",
    "                continue\n",
    "\n",
    "            # Save the image as PNG\n",
    "            image = np.transpose(image, (1, 2, 0)).astype(np.uint8)  # Ensure correct shape and type\n",
    "            Image.fromarray(image).save(img_path)\n",
    "\n",
    "            # Save the depth and surface normals as .npy to preserve raw values\n",
    "            np.save(depth_path, depth)\n",
    "            np.save(norm_path, snorm)\n",
    "\n",
    "            # Save the room type and NYU index as .npy metadata\n",
    "            metadata = {\"room\": room, \"nyu_index\": nyu_index}\n",
    "            np.save(meta_path, metadata)\n",
    "\n",
    "        return {\"status\": \"success\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the file as a bad one\n",
    "        with open(bad_files_log, 'a') as log_file:\n",
    "            log_file.write(f\"Error processing test set: {e}\\n\")\n",
    "        print(f\"Error processing test set: {e}\")\n",
    "        return {\"status\": \"failure\"}\n",
    "\n",
    "def process_nyuv2_test(path, output_dir, bad_files_log=\"bad_test_files.log\"):\n",
    "    \"\"\"\n",
    "    Load and process NYUv2 test set from .pkl file.\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        data_dict = pickle.load(f)\n",
    "\n",
    "    # Use tqdm for tracking the progress of loading data\n",
    "    indices = data_dict[\"test_indices\"]\n",
    "    depths = [data_dict[\"depths\"][_i] for _i in tqdm(indices, desc=\"Loading depths\")]\n",
    "    images = [data_dict[\"images\"][_i] for _i in tqdm(indices, desc=\"Loading images\")]\n",
    "    snorms = [data_dict[\"snorms\"][_i] for _i in tqdm(indices, desc=\"Loading surface normals\")]\n",
    "    scenes = [data_dict[\"scene_types\"][_i][0] for _i in tqdm(indices, desc=\"Loading scene types\")]\n",
    "\n",
    "    test_set = list(zip(images, depths, snorms, scenes, indices))\n",
    "\n",
    "    return extract_nyuv2_test_data(test_set, output_dir, bad_files_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the NYUv2 .pkl file and the output directory\n",
    "nyuv2_pkl_path = \"/p/openvocabdustr/probing_midlevel_vision/code/probing-mid-level-vision/data/nyuv2/nyuv2_snorm_all.pkl\"\n",
    "output_dir = \"/p/openvocabdustr/probing_midlevel_vision/code/probing-mid-level-vision/data/nyuv2_test_processed\"\n",
    "\n",
    "# Process the NYUv2 test set and extract data\n",
    "process_nyuv2_test(nyuv2_pkl_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load train and test datasets\n",
    "# train_dataset = load_dataset('parquet', data_files='/p/openvocabdustr/probing_midlevel_vision/code/probing-mid-level-vision/data/nyu_geomet_parquet_seg/train_nyu_dataset.parquet')\n",
    "# test_dataset = load_dataset('parquet', data_files='/p/openvocabdustr/probing_midlevel_vision/code/probing-mid-level-vision/data/nyu_geomet_parquet_seg/test_nyu_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm  # Importing tqdm for progress bar\n",
    "\n",
    "\n",
    "def save_dataset_in_segments(dataset, segment_size, save_dir, split_name):\n",
    "    \"\"\"\n",
    "    Saves a dataset in segments to separate Parquet files using Hugging Face `datasets`.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset to be saved.\n",
    "        segment_size: The size of each segment (number of samples).\n",
    "        save_dir: Directory to save the segments.\n",
    "        split_name: Name of the split (train/val/test) to organize the files.\n",
    "    \"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Calculate the number of segments\n",
    "    num_segments = len(dataset) // segment_size + (1 if len(dataset) % segment_size != 0 else 0)\n",
    "\n",
    "    # Save each segment as a separate Parquet file\n",
    "    for i in tqdm(range(num_segments), desc=f\"Saving {split_name} segments\"):\n",
    "        start_idx = i * segment_size\n",
    "        end_idx = min((i + 1) * segment_size, len(dataset))\n",
    "\n",
    "        # Extract the segment\n",
    "        segment = dataset.select(range(start_idx, end_idx))\n",
    "\n",
    "        # Define the file path for each segment\n",
    "        segment_file = os.path.join(save_dir, f\"{split_name}_segment_{i}.parquet\")\n",
    "\n",
    "        # Save the segment as a Parquet file\n",
    "        segment.to_parquet(segment_file)\n",
    "\n",
    "        print(f\"Saved segment {i} with {end_idx - start_idx} samples to {segment_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for saving\n",
    "segment_size = 100  # Define how many samples per segment\n",
    "save_dir = \"/p/openvocabdustr/probing_midlevel_vision/data/nyu_geonet_segmentation_parquet_v2\"  # Directory to save the Parquet files\n",
    "split_name = \"train\"  # Name of the split\n",
    "\n",
    "# Save the train_normal_dataset in segments\n",
    "save_dataset_in_segments(train_dataset[\"train\"], segment_size, save_dir, split_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for saving\n",
    "segment_size = 100  # Define how many samples per segment\n",
    "save_dir = \"/p/openvocabdustr/probing_midlevel_vision/data/nyu_geonet_segmentation_parquet_v2\"  # Directory to save the Parquet files\n",
    "split_name = \"val\"  # Name of the split\n",
    "\n",
    "# Save the train_normal_dataset in segments\n",
    "save_dataset_in_segments(test_dataset[\"train\"], segment_size, save_dir, split_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "# Initialize the HfApi object and ensure authentication\n",
    "api = HfApi()\n",
    "\n",
    "# Ensure that you are logged in or provide the token\n",
    "token = HfFolder.get_token()\n",
    "if token is None:\n",
    "    raise ValueError(\"You need to log in to Hugging Face or provide an access token!\")\n",
    "\n",
    "def get_uploaded_files(repo_id, token):\n",
    "    \"\"\"\n",
    "    Get a list of files already uploaded to the Hugging Face Hub.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List the files in the repository\n",
    "        files_info = api.list_repo_files(repo_id, repo_type=\"dataset\", token=token)\n",
    "        return set(files_info)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retrieve uploaded files: {e}\")\n",
    "        return set()\n",
    "\n",
    "# Define a function to push a dataset segment to the Hub\n",
    "def push_segment_to_hub_hfapi(segment_file, repo_id, token, commit_message, progress_counter, total_files):\n",
    "    # Upload the segment to the repository\n",
    "    try:\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=segment_file,  # Local file path\n",
    "            path_in_repo=os.path.basename(segment_file),  # The path in the repo\n",
    "            repo_id=repo_id,  # Repo name on Hugging Face Hub\n",
    "            repo_type=\"dataset\",  # Specify that it's a dataset\n",
    "            token=token,\n",
    "            commit_message=commit_message\n",
    "        )\n",
    "        progress_counter['uploaded'] += 1  # Update the counter\n",
    "        print(f\"Successfully uploaded {segment_file} ({progress_counter['uploaded']}/{total_files})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {segment_file} due to: {e}\")\n",
    "\n",
    "def push_dataset_segments_in_parallel(dataset_dir, repo_id, num_threads=4):\n",
    "    # Get the list of already uploaded files\n",
    "    uploaded_files = get_uploaded_files(repo_id, token)\n",
    "\n",
    "    # List all segment files (e.g., Parquet files) in the dataset directory\n",
    "    segment_files = [os.path.join(dataset_dir, f) for f in os.listdir(dataset_dir) if f.endswith(\".parquet\")]\n",
    "\n",
    "    # Filter out the files that are already uploaded\n",
    "    files_to_upload = [f for f in segment_files if os.path.basename(f) not in uploaded_files]\n",
    "\n",
    "    total_files = len(files_to_upload)\n",
    "    \n",
    "    if total_files == 0:\n",
    "        print(\"All files are already uploaded!\")\n",
    "        return\n",
    "\n",
    "    # Counter to track the number of uploaded files\n",
    "    progress_counter = {'uploaded': 0}\n",
    "\n",
    "    print(f\"Starting upload of {total_files} files...\")\n",
    "\n",
    "    # Use ThreadPoolExecutor to push segments in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for segment_file in files_to_upload:\n",
    "            commit_message = f\"Uploading {os.path.basename(segment_file)}\"\n",
    "            futures.append(executor.submit(push_segment_to_hub_hfapi, segment_file, repo_id, token, commit_message, progress_counter, total_files))\n",
    "\n",
    "        # Ensure all threads are completed\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error during uploading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"/p/openvocabdustr/probing_midlevel_vision/data/nyu_geonet_segmentation_parquet_v2\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"uva-cv-lab/nyu_geonet_segmentation_parquet\"\n",
    "dataset_dir = \"/p/openvocabdustr/probing_midlevel_vision/data/nyu_geonet_segmentation_parquet_v2\"\n",
    "\n",
    "# Call the function to push dataset segments in parallel\n",
    "push_dataset_segments_in_parallel(dataset_dir, repo_id, num_threads=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
